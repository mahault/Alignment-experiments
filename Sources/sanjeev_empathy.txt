Transcript
Mahault Albarracin: [00:02] Hello, Anna. [00:03] How are you doing?
Anna Mikeda: [00:07] I'm good, Sanjeev. [00:09] Very nice to meet you. [00:11] Nice to meet you as well.
Mahault Albarracin: [00:15] Awesome. [00:17] So I guess we're waiting for Allegra maybe. [00:20] And is there someone else? [00:23] I think none of them said they were coming. [00:25] They didn't confirm. [00:28] So I think we can probably just continue record and send it to them. [00:33] I think that's what we're going to do. [00:41] You want to go? [00:42] Sandeep, I think you're the one who organized this.
Sanjeev Namjoshi: [00:44] Yes, sorry, I was just pulling up something here. [00:46] Okay, cool. [00:47] All right, let me go ahead and share my screen. [00:49] I prepared some slides. [00:51] So the kind of point of the slides I want to. [00:54] The project has gone through so many different iterations, and I know that I haven't really touched the project in three months because of the furlough, I was unable to get my notes and code or anything from my laptop. [01:08] I'm going off of my memory purely here. [01:11] I'll give just the context of where the project was left off and then where I think we should go next. [01:18] But we should have a discussion about scoping. [01:20] So I'm going to kind of put everything on the table at first, and then we can kind of decide what the right move is here. [01:27] So here's my slides.
Mahault Albarracin: [01:31] All right.
Sanjeev Namjoshi: [01:33] Okay. [01:33] Can you see my slides?
Mahault Albarracin: [01:35] Yeah.
Sanjeev Namjoshi: [01:36] All right, cool. [01:37] So this shouldn't take too long. [01:40] So I don't know who was on the original paper from last year. [01:45] There was a philosophy paper. [01:47] I know Mao was on it. [01:49] I don't know who else was on it. [01:50] But that's essentially where this all kind of started, was to add a active inference model to the philosophy paper. [01:59] But in that process, it looked like the paper would probably be rewritten. [02:02] And so there's lots of information and chunks of ideas out there that I think need to be put back together in a coherent way right now. [02:08] But there's lots of content around. [02:10] So I think that's one of the challenges that will be part of this. [02:16] So these are kind of just general aims for an active inference model. [02:19] There's a lot more, you know, we've sort of talked about, but just as a general aim, one important thing we wanted to see in this model was instead of cooperation being something that's kind of, you know, hard coded into the active inference model, we want that behavior to be emergent. [02:36] So using however we define empathy in the active inference model, we want cooperation to be a result of certain settings of what empathy is in the model. [02:46] So when we mess with the empathy of the two agents, there are certain conditions under which cooperation emerges. [02:52] So then we can start to say interesting things about the nature of cooperative behavior and the way that active inference models learn each other. [03:00] In this case. [03:01] Sort of shared environment. [03:05] One other part of it is the theory of mind aspect, which is key here is part of the empathy idea that you can infer the beliefs of other agents in response to your actions. [03:16] So if you, if you were to do something, you could infer what would the other agent do in response to me and then what action would they take as a response and then how would I react to that. [03:26] So obviously you can keep, you know, iterating through that, but keeping it at the simplest stage zero of that. [03:32] So just the first back and forth which is just enough to know how the other person's going to behave. [03:40] We had discussed factoring in emotional state and I'll explain how in the model this was proposed to be done. [03:47] But I would call the emotional state more of like a, a bit of a, like a label on what's going on in the, in the VFE IFE calculation. [03:57] So but it's something we can analyze post hoc and we can say like what was the state of the agent if we define emotion in this way. [04:04] And I'll talk about that in a second how that would work. [04:07] So there could be interesting analysis there too. [04:10] And then the kind of the main star I think of the model is really how empathy is controlled in the model. [04:16] So what is this parameter and how does it control the degree to which an agent takes into account their beliefs versus the beliefs of the other agent in the simulation? [04:28] So I'll stop there for a second just before I go into the modeling details just to make sure that that's clear so far. [04:35] I'll go into more detail, but does that kind of make sense?
Mahault Albarracin: [04:37] Anna?
Sanjeev Namjoshi: [04:40] Excellent. [04:40] So the game or task that we had chosen was the prisoner's dilemma. [04:49] Are you familiar the prisoner's dilemma, Anna? [04:51] Okay, yeah. [04:53] So I think the biggest take home message in this game for at least our context is that if you look at the problem from the perspective of rational self interested behavior, then confessing is the best option because you know, you don't care about what the other person, what happens to the other person. [05:11] But what makes this problem interesting is of course that collective behavior is what is more interesting. [05:16] Where it could turn out that if you do not act in a self interested way and you do care about the other person and the other person also cares about you, then the cooperative option is actually the better option overall, which is the one year option here on the bottom. [05:31] Right. [05:32] So this kind of gives an interesting scenario because this game is usually not played as a one shot or it's usually played as a one shot, but in the iterated version, right, you can see you can learn the behavior of other agents or the other agent in the simulation over time. [05:47] And the hope would be that the empathy factor would control some level of how fast or other types of dynamics about how the cooperation would emerge as being the best option as the agents mutually learn from each other over time. [06:03] So that's kind of what. [06:04] That's what the code is going to look like. [06:07] So a lot of this came from. [06:10] As a starting point, there is an active inference model of the iterated prisoners dilemma. [06:15] And this paper I really recommend reading. [06:17] I have references at the end of the slides that I can share. [06:21] And this one is essentially kind of what we're, what we've been building off of, but then adding in the empathy layer on top of that. [06:29] Because in this paper they primarily looked at learning rates on the B matrix to see how fast it takes the agents to learn the transitions and then start to cooperate with the other agent. [06:45] So in this model, the way that it's set up is that we only have two agents and so these different categories in these different states basically in this vector here for the hidden states and the observations correspond to the action of each player. [07:06] So player one is always in the first position here and player two is in the second. [07:10] So this is either cooperate or defect. [07:15] So this means this is the circumstance where the agent observes that it just cooperated in the last round and, and the other agent also just cooperated in the last round. [07:25] So you're observing what happened the last time this game was played and other observations they could see is that they cooperate in the last round of the other player defected and so on. [07:35] So it just exhaustively computes the different combinations of the two actions that are possible, which is to either cooperate or defect. [07:43] And you observe the action at the end of each round. [07:50] And this is an identity mapping to hidden states. [07:53] So an A matrix here would just be just the diagonal and sometimes a little bit of noise might be added just a tiny bit to make some stochasticity in there. [08:02] And so the states are the same thing basically. [08:05] So they're just. [08:06] There's no hidden. [08:08] Everything is fully observed in the scenario is what this would mean. [08:13] So we have our observation model which is just going to be a categorical distribution parameterized by that identity matrix. [08:22] The transition is just going to compute the different possible actions could do given that they have just done, given what the other agent has done. [08:33] So because the identity matrix is. [08:35] Or the B matrix has states transitioning, it's saying if we both cooperated last time, then what is the probability of one of these other options or the same option for the next time step? [08:50] So that's something that the agent has to learn. [08:52] So that's why there's a transition model parameter here we have a prior over the V matrix so we can learn what are those transition probabilities over time, and that's how the agents basically learn each other's behaviors. [09:05] Through that then there's just the initial state prior. [09:09] And then in order to encode the payoff matrix from the Prisoner's Dilemma, we use this particular set of logits 3142 which comes from this paper as well. [09:24] And that's just showing you that same cooperate defect matrix from before where we've encoded the rewards in logits here in the C vector and that so for its agent specific. [09:35] So 3142 is the focal agents probabilities in the first positions here. [09:40] So that corresponds to this 314 2. [09:44] Okay, so that's, that's the setup of the model. [09:46] Do you have any questions about that part?
Anna Mikeda: [09:52] Oh good, excellent.
Sanjeev Namjoshi: [09:55] Okay, so here is what the model looked like. [10:00] And the last thing that I talked about with one of a former coworker who was working on this model was actually changing it to changing it in some way that I'll talk about. [10:10] So I want to explain first what we were doing and then what we were changing it to. [10:15] And also at that point after I finished presenting, it'd be good to talk mal for us to talk about what you mentioned someone else who's been working on the code. [10:24] I don't know where they picked up from. [10:27] So I think that's something that needs to be clarified as well. [10:29] So this is how we'd originally proposed to do this model. [10:34] So you have two agents here, you could have more. [10:36] I mean it's possible to extend this as far as you want, but just two for simplicity for now. [10:41] So each agent has its own model which we call the self model. [10:46] Sometimes like the focal agents model is what the word is used. [10:49] And you could calculate EFE and VFE from that model and that would be just using the ABC and D. [10:56] That's your model. [10:57] It's all here, it's all specified so that agent has some settings for their model from which they can get EFE and bfe. [11:05] Now what it does is it spawns a copy of that model and assumes that the other agent is basically the same as me. [11:12] It would behave like I would. [11:14] That's an assumption. [11:15] We could of course make more complex scenarios, but that's the starting point. [11:20] And then we would say, well based on what I believe the other agent is doing, what would its EFE and VFE be? [11:27] And the way that we now include empathy here is that it's all through weighting of expected free energy. [11:33] So you have calculated your own expected free energy and you've also calculated what you believe the expected free energy of the other agent is, which may or may not be true. [11:42] It's all about your assumptions in your model. [11:45] And so then you can take the expected value of that, where you weight the contribution of the overall efe, weighting toward how much are you taking into account your EFE versus the other agent's EFE. [11:58] So if it's like a 9 to 1 ratio of self to other, then you're effectively being selfish. [12:05] In that scenario, whereas if you reverse it and it's 0.1, 0.9 for example, or 01, whatever, you know, heading toward the other direction, then you are having a weighting which is more altruistic. [12:17] So you have kind of a sliding scale. [12:18] Now that you could look at this overall efe which is now weighted and has taken into account the other agents beliefs using EFE as this sort of summary metric of their overall state in that moment, then we can then take actions from that where we have now the action selection now takes into account the belief about the other agent. [12:42] And likewise we could do this with VFE as well. [12:45] And following this paper or perhaps other approaches, we could calculate about valence and arousal from these quantities and then be able to compute an emotion using a circumplex model looking at the axis of where valence and arousal, what point you're in in that particular space depending on how you've calculated your EFE and bfe.
Anna Mikeda: [13:10] So go ahead. [13:12] I do have a question here. [13:13] This is also a question from Mao because similar model was in her paper about the emotional circumplex. [13:21] Why do you not use dominance? [13:25] Because sometimes when there is for example negative balance and high arousal, it is. [13:31] It can be both fear or anger depending on dominance. [13:35] So would you like. [13:38] Why do you not use it?
Sanjeev Namjoshi: [13:39] Dominant?
Mahault Albarracin: [13:40] Yeah. [13:41] So we haven't quite mapped some of the differences. [13:46] Like the circumflex model was extremely simplistic and if you look at the results, the agents never go to emotions like anger. [13:54] Right. [13:54] Because there's a lack of granularity there. [13:58] The assumption is that potentially the tenor of the emotion has to do with something you're talking about. [14:10] But in terms of efe, dominance isn't really a thing. [14:14] So we'd have to couch that term under. [14:17] Well, there's a specific set of precision over your policies that is being foiled. [14:25] You're getting a lot of error relative to your policies and that causes anger rather than fear, something like that. [14:33] But we haven't tested that yet. [14:35] And we'd have to couch dominance under those terms because we have a paper about power and we have a paper about empowerment, which are slightly different. [14:47] So I think there could be a way to recast some of it. [14:49] And if that's something you're interested in, we could do it. [14:52] The point is we lack granularity currently in the emotional.
Sanjeev Namjoshi: [14:57] In the emotional, As far as I know, like, you know, I propose this part on the right. [15:05] Right here. [15:06] But it wasn't something that Mao and I have talked about in a ton of detail yet, or at least it's been months since we've come back to that question. [15:11] So, I mean, you know, this part here could be changed in some way or discussed. [15:15] I think that's such an open question. [15:17] And the paper I think could work without including this. [15:20] It sort of just depends on what we want to do. [15:23] That's kind of, I think, the big question here. [15:24] So I think we should come back to this piece at some point after I finish the slides. [15:30] That could be another topic of discussion. [15:34] So, yeah, there's a couple other things here. [15:36] So like there's the A matrix is stochastic, which just means its identity, but you have a bit of inverse temperature noise on it. [15:43] There's learning. [15:44] For a B matrix, we talked about learning preferences. [15:47] So you don't know exactly how the other agent, what, what its preference matrix is, but it's something we did not enable and that may also be more complex than we want as a first model. [15:58] This could be many papers, possibly as we add more detail and complexity. [16:03] So here is, this is from the other paper that I mentioned from Daphne and Connor using. [16:11] They did not use empathy, but they had a similar sort of setup. [16:15] And so there, there we're looking at the learning rate for the B matrix, but I think we can do something very similar for the empathy parameter. [16:24] So you would have basically a grid search over different empathy factors instead of learning rates here, and then some kind of patterns of behavior, of cooperation. [16:35] You would expect some kind of a structure to emerge. [16:37] That's sort of what I'm expecting to see depending on where the settings are for the empathy factor for the, for the two agents, for example, so when do you see cooperation? [16:51] And then there are lots of other questions we could ask. [16:54] You know, examining the emotional state, looking at within without being learning, how does that affect things? [16:59] How does preference parameter affect if we learn preferences, does that change how empathy behaves? [17:07] If a matrix is more complicated in some way, there's more noise. [17:10] You know, there's lots of little details. [17:12] And the bigger, I think, really interesting one, which could be for the later on in the future, is expanding to multiple agents. [17:17] I think that could be very interesting as you could probably get clicks and other kinds of emergent behaviors that would come out of those sorts of interactions. [17:29] So that is all current up to around, I would say maybe like August. [17:33] I want to say something around maybe July. [17:35] August. [17:36] And then after discussion with some employers, researchers at versus one thing that was brought up was that when you are calculating expected free energy for each agent here, are these expected free energies actually the same thing? [17:56] So the question was if these two models, if you. [18:01] You're copying your model, but there may be still differences. [18:05] In what the models are actually referring to. [18:09] So there are different policies within each of these that are calculated from the model. [18:14] But if the models are not have this do not have the same exact structure, then averaging EFE doesn't really make sense because there are what one policy means for this agent and the belief for the other agent may be different policy spaces. [18:32] So their particular policies may actually not completely we're labeling the same. [18:36] We can call them policy 1, 2, and 3, but mathematically they're not actually equivalent structurally in the space of the policy distribution. [18:45] And so as a result of that, averaging would need to be very careful, carefully done. [18:52] And it was unclear exactly how that could be done. [18:56] There is probably a way to do it, but it would probably get more involved. [19:00] And so the suggestion was, which is where we had started going in the next direction was using the sophisticated inference tree from this theory of mind paper from another colleague at versus and replace basically that whole unit with this approach. [19:21] And so in their approach, what they do is it's pretty involved. [19:25] So I'll just give the basic idea. [19:26] But under sophisticated inference, you actually have a tree where you expand out your beliefs, starting first, and then you from there start to expand out. [19:41] Okay, if I did that action, what would be my new updated state then? [19:45] If I did that, what would the other agent do in response to that? [19:49] And then what would the other agent observe as a result of that? [19:52] And then you keep doing that, unpacking this tree going down the line here until you get to a certain point and then a goal point. [20:00] And then you move backward through that tree, calculating expected free energy with backward induction until you get to where you are right now. [20:08] And so then the idea is that, you know, if you want to go through this chain of, well, if I did this, they would do this. [20:13] I would do this and end up in this final state. [20:16] Here is the policy I would need to enact in order to get through that tree. [20:20] And the nice thing is they put all this in code already. [20:22] So the coding part would be very easy. [20:25] It would simply be about adapting it to the prisoner's dilemma. [20:28] So it's not difficult. [20:30] I think this is a much more sophisticated machinery than what I had been originally proposing. [20:37] That's another point we need to discuss again, because my understanding was last time we talked, that was the direction that my colleague Philip was going, and he worked on that and he'd beginning. [20:48] Starting started work on that before other priorities took over. [20:52] So there's definitely a question about if this is what we want to pursue. [20:57] And if you do that, expected free energy becomes more complex because it has to be calculated recursively. [21:03] And again, that's all in the code. [21:05] So the question would just be about how would averaging work in the same. [21:08] Scenario. [21:09] So we could put in the empathy factor in here by waiting EFE through the expansion through the tree. [21:15] So that when you're considering the EFE contribution from the focal agent yourself versus the other model, when you're going backward, you do the weighting in there. [21:25] So the overall EFE in this equation here is calculated from the contributions that are weighted as you recursively climb up back up the tree again. [21:35] So all those recursive contributions as you go up here, if you're dealing with these red nodes, that's the, that's the focal agent. [21:43] If you're dealing with purple, then it's the other agent. [21:46] So the EFE contributions for each of these sections could be weighted.
Anna Mikeda: [21:51] Go ahead. [21:52] But this assumes that you have perfect knowledge of the other agent and his inner workings.
Mahault Albarracin: [21:57] Or no. [21:57] Yes.
Anna Mikeda: [21:58] Now.
Sanjeev Namjoshi: [21:58] Yes, that's correct. [22:00] Yeah. [22:01] Yeah, thank you for bringing that up. [22:02] Yeah, that's exactly right. [22:04] So that's a different sort of situation. [22:05] Right. [22:06] So this is how I think we kind of have a choice here. [22:08] We need to sort of figure out what is the right approach.
Mahault Albarracin: [22:13] You'll find that it's actually a problem not because it sets up the perfect setup and that's unrealistic, but more because it doesn't have any symmetry breaking. [22:24] And if you don't have any symmetry breaking, then the agents get stuck in a recursive loop of prediction where I think you're going to do the thing I would do and therefore I'll do the other thing. [22:36] And they both predict the same thing. [22:38] So they both do the other thing, which is not the thing they would have predicted. [22:43] So you get into this recursive problem and that only happens if it's the same model. [22:50] If it's an asymmetrical model, suddenly there is no problem. [22:55] I predict you're going to do this other thing and you predict I'm going to do this thing. [22:58] And that's correct because we're not the same. [23:00] So that's. [23:01] It sort of bakes in. [23:07] It sort of bakes in an asymmetry and a turn taking. [23:12] But. [23:13] And so based on that, what you can do is not have this full certainty over what is the other model of the other agent. [23:20] You can have a sort of probability distribution over what they might do and then suddenly it really is an inference problem.
Sanjeev Namjoshi: [23:31] Thank you, Mal. [23:32] Yeah, that's exactly right. [23:35] So that brings us to just kind of the sort of scoping and options in front of us. [23:41] I don't know as I just don't even have access to verses right now. [23:44] What is if we can access that theory of mind code. [23:48] I'm not really sure how that would work. [23:51] So that's an open question and is the kind of scoping is just to stop at emergence. [23:58] So we have a two player game, it's iterated, we have an empathy parameter identity, A matrix, B matrix learning, but there's no emotion component. [24:05] And so that's kind of the simplest sort of vanilla version we could do. [24:10] Which is maybe a good starting point. [24:11] The question is, is that enough? [24:13] Can we get enough analysis out of that, where we just look at the empathy parameter and we say, you know, here are the settings under which cooperation emerges. [24:22] Or do we want to add, you know, all these other kind of following other things that I had mentioned before, other things we could do. [24:28] I don't know where the right scope is. [24:31] And there's also the question of, like, yeah, is this. [24:34] Is this theory of mind code the best way to go? [24:37] Is it available? [24:38] Those are some of the other questions as well. [24:41] So, yeah, that's kind of my understanding where things are at. [24:43] Maybe things have evolved since I have been gone.
Mahault Albarracin: [24:45] I don't know. [24:47] I think we should start with the most basic scope. [24:51] And then if it's easy, then we should add the emotional state, just as a sort of analysis. [24:57] Because the emotional state effectively is already impacting the behavior because you're factoring the other EFE in your own EFT computation. [25:06] So you, as you do the rollout, you have a simulation. [25:10] That simulation affects you. [25:12] That's it. [25:13] That's. [25:14] That's the. [25:15] That's. [25:16] That's the emotional state. [25:17] The problem is we're not really showing the understanding of that emotional state. [25:23] But for now, I think maybe that's fine. [25:26] Like, you know what I mean?
Sanjeev Namjoshi: [25:30] Like, building on the paper. [25:31] That's fine for a starting point. [25:32] We could have separate papers where we expand that out, is what you mean.
Mahault Albarracin: [25:35] Yeah, yeah. [25:38] The core thing is I sent you the code. [25:41] Is it sent? [25:42] Actually, no, it's not sent. [25:46] For some reason, I sent you some code, Sanjeev, but it says neither read nor delivered. [25:58] I don't know why. [25:59] I can send it again. [26:00] Try to send it again. [26:01] But it was the empathy code, basically. [26:05] Let's see. [26:05] Is it gonna work? [26:06] For some reason, it's just not sending you this stuff. [26:09] I could send it to you by email. [26:12] How are you sending it by email?
Sanjeev Namjoshi: [26:16] Was it just like, straight from a GitHub link or was it like a.
Mahault Albarracin: [26:20] Like a. [26:21] No, it's the zip.
Sanjeev Namjoshi: [26:22] It's the zip file.
Mahault Albarracin: [26:23] Okay. [26:25] What's the email?
Sanjeev Namjoshi: [26:26] I can reach you at. [26:29] It should be sanjeev namjoshimail.com which is the one I've been using, we've been corresponding with.
Mahault Albarracin: [26:35] Right. [26:35] I'm surprised it's not going through at Gmail, because it's not. [26:40] No, no, it's fine. [26:41] I was sending it to you on WhatsApp. [26:48] Communicating most, but file is zero bytes and cannot be attached. [26:53] What the. [26:54] Maybe that's the problem. [26:58] Okay. [26:59] I'll have to download it again. [27:01] But the point is, I'm going to send you this code. [27:02] You can maybe take a look and see if there's anything you need in there that you can take a look. [27:07] That's what I sent us to. [27:11] Hongju. [27:12] So that's what she ran. [27:13] So I'm going to make sure you know what it is she ran. [27:17] I can do a really big chunk of the. [27:20] Of this. [27:20] I've already been working with the empty code and theory of mind on a different experiment. [27:27] On an experiment where two agents have to get to their goal and navigate a tricky environment and stuff. [27:33] And it's working really well. [27:35] So I can send you that as well if you want to take a look at it. [27:37] But I was. [27:38] I know we wanted to do the Prisoner's Dilemma, so it's up to you. [27:45] I can do a big chunk of it. [27:48] Anna, I don't know what you want to do, but maybe you can help me draft the paper. [27:52] I know Hongju wanted to do that as well. [27:56] Andrea wanted to do that as well. [27:58] So basically what would matter is to write the paper, outline, produce the correct experiments such that it's meaningful and useful, and then, you know, submit. [28:09] Basically, it's not a. [28:11] We've got most of the pieces in place now. [28:13] We have a lot of content. [28:15] We've got the code and the model, etc. [28:18] So Hongju also added something. [28:21] So basically I'm going to send them this recording and see what she's added to see if there's anything that she can tell you that would also contribute to your understanding. [28:34] Because I know you had some questions and maybe she can help answer some of them. [28:38] Does that work for everyone?
Sanjeev Namjoshi: [28:42] Yeah, that works. [28:44] I would like to just say, like, in terms of timing, when I was at Versus, I could work on it during work time, but I have too much going on for outside versus work, so I won't be able to contribute very much. [28:56] I can go for these meetings for managerial and discussions. [29:00] I can also help write technical material and some of the paper editing and that kind of stuff. [29:05] But I won't have time to work on the code.
Mahault Albarracin: [29:06] That's okay. [29:07] That's all right. [29:08] That's why Hong and I are going to be on it and Anna as well, if you're interested. [29:14] So. [29:15] Yeah, I think it'll be fine from there.
Sanjeev Namjoshi: [29:18] Okay.
Anna Mikeda: [29:21] Yeah. [29:22] I would add Mao, if you can send me the code as well. [29:26] I mean, unless it's like versus protected.
Mahault Albarracin: [29:28] Yeah, no, no, no, no.
Anna Mikeda: [29:31] I would have a look as well. [29:34] And yeah, I'm very happy to help with the writing of the paper. [29:40] I would really hope that after we have the simple version, we can also move to the more emotional side because that's kind of my stronger side. [29:49] But we can leave this to see that we have resource in the end.
Mahault Albarracin: [29:56] Okay, this sounds perfect.
Anna Mikeda: [30:00] Anna.
Mahault Albarracin: [30:01] Nikita, what's your email?
Anna Mikeda: [30:03] Anna, it's Nikida, 17. [30:11] I can write it here.
Mahault Albarracin: [30:15] Okay.
Anna Mikeda: [30:15] Yeah.
Mahault Albarracin: [30:17] So I'm forwarding the code to you as well, so you have it.
Anna Mikeda: [30:24] Making sure that everybody's on the same page, but. [30:28] Okay.
Mahault Albarracin: [30:30] All right, well, thanks, everyone. [30:34] We probably will reschedule one of these in a little bit once we've got a little bit of progress. [30:39] Sanjeeva can also send you the repo where I've been working on the empathy stuff, so if you want to take a look, you don't have to, but you're welcome to. [30:46] And so, yeah, I'll send a bunch of potential tasks in the email thread.
Sanjeev Namjoshi: [30:52] Cool. [30:53] Excellent.
Mahault Albarracin: [30:55] Bye, everyone.
Sanjeev Namjoshi: [30:56] Nice meeting you, Anna.