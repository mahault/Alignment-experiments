{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing ToM planning algorithms with the Englishmen Scenario in the Overcooked (v1) task\n",
        "\n",
        "Karl described a scenario where two polite Englishmen approaching a door at the same time would each defer to the other, resulting in neither actually going through. It's a coordination failure caused by mutual courtesy.\n",
        "\n",
        "We're generalising this to the Overcooked environment where two agents go for the same resource (an onion) at a central location. There is an environmental constraint that agents can't occupy the same location. By varying the agents' ToM capabilities, we test three conditions:\n",
        "\n",
        "2 non-ToM Agents: Both agents select the action to move to the centre location to access the onion pile, resulting in a deadlock and task failure.\n",
        "\n",
        "1 non-ToM + 1 ToM Agent: The ToM agent predicts the non-ToM agent will select the action to move to the centre, so it selects the action to stay instead, resulting in task completion.\n",
        "\n",
        "2 ToM Agents: Both agents predict the other will select the action to move to the centre, so they both select the action to stay, mirroring the Englishmen scenario and resulting in neither agent moving and task failure.\n",
        "\n",
        "Note: Using clipped model for simpler scenario to experiment with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax.random as jr\n",
        "import jax.tree_util as jtu\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# from tom.models import OvercookedModel, OvercookedAgent\n",
        "from tom.models.model_ocv1_clipped import OvercookedModel, OvercookedAgent\n",
        "\n",
        "from jaxmarl.environments.overcooked import overcooked_layouts, layout_grid_to_dict\n",
        "from tom.envs.env_ocv1_clipped import OvercookedV1Env\n",
        "\n",
        "from jaxmarl.viz.overcooked_visualizer import OvercookedVisualizer\n",
        "\n",
        "from tom.planning.si import si_policy_search as si_policy_search_nontom\n",
        "from tom.planning.rollout import rollout as rollout_nontom\n",
        "\n",
        "from tom.planning.si_tom import si_policy_search_tom, ToMify\n",
        "from tom.planning.rollout_2tom import rollout as rollout_tom\n",
        "\n",
        "from tom.planning.visualize import visualize_plan_tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialise the environment\n",
        "\n",
        "- W = wall\n",
        "- O = onion pile\n",
        "- P = pot\n",
        "- A = agent\n",
        "- B = plate pile\n",
        "- X = delivery station\n",
        "- empty spaces are empty cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_layout_grid = \"\"\"\n",
        "WWOWW\n",
        "WA AW\n",
        "P   B\n",
        "WWXWW\n",
        "\"\"\"\n",
        "layout = layout_grid_to_dict(custom_layout_grid)\n",
        "\n",
        "# # or if you want to use a pre-set layout\n",
        "# layout = overcooked_layouts[\"cramped_room\"] # options: cramped_room, asymm_advantages, coord_ring, forced_coord, counter_circuit\n",
        "\n",
        "num_agents = 2\n",
        "timesteps = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = OvercookedV1Env(\n",
        "    num_agents, layout, timesteps, \n",
        "    initiate_inventory=None, initiate_facingdir=[2, 3]\n",
        "    )\n",
        "# env = OvercookedV1Env(num_agents, layout, timesteps, initiate_inventory=[\"onion\", \"empty\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = jr.PRNGKey(1)\n",
        "obs, state = env.reset(key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialise pymdp agents' generative model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = OvercookedModel(env_layout=layout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialise the non-ToM and ToM planning algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "horizon=2\n",
        "max_nodes = 20000\n",
        "max_branching = len(model.B[0].batch[\"actions\"])\n",
        "policy_prune_threshold = 0.0\n",
        "observation_prune_threshold = 0.0\n",
        "entropy_stop_threshold = 0.5\n",
        "efe_stop_threshold = 10\n",
        "kl_threshold=-1\n",
        "prune_penalty = 512\n",
        "gamma = 8.0\n",
        "topk_obsspace = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up the policy search function\n",
        "tree_search_nontom = si_policy_search_nontom(\n",
        "        horizon=horizon,\n",
        "        max_nodes=max_nodes,\n",
        "        max_branching=max_branching,\n",
        "        policy_prune_threshold=policy_prune_threshold,\n",
        "        observation_prune_threshold=observation_prune_threshold,\n",
        "        entropy_stop_threshold=entropy_stop_threshold,\n",
        "        efe_stop_threshold=efe_stop_threshold,\n",
        "        kl_threshold=kl_threshold,\n",
        "        prune_penalty=prune_penalty,\n",
        "        gamma=gamma,\n",
        "        topk_obsspace=topk_obsspace\n",
        "    )\n",
        "\n",
        "tree_search_tom = si_policy_search_tom(\n",
        "            horizon=horizon,\n",
        "            max_nodes=max_nodes,\n",
        "            max_branching=max_branching,\n",
        "            policy_prune_threshold=policy_prune_threshold,\n",
        "            observation_prune_threshold=observation_prune_threshold,\n",
        "            entropy_stop_threshold=entropy_stop_threshold,\n",
        "            efe_stop_threshold=efe_stop_threshold,\n",
        "            kl_threshold=kl_threshold,\n",
        "            prune_penalty=prune_penalty,\n",
        "            gamma=gamma,\n",
        "            other_agent_policy_search=tree_search_nontom\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper functions to print actions and convert environment sequence for visualisation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_actions(actions):\n",
        "    action_names = model.B[list(model.B.keys())[0]].batch['actions']\n",
        "    \n",
        "    for agent_idx, agent_actions in enumerate(actions):\n",
        "        print(f\"AGENT {agent_idx} ACTIONS:\")\n",
        "        \n",
        "        for t in range(agent_actions.shape[0]):\n",
        "            action_idx = agent_actions[t][0] if agent_actions[t].ndim > 0 else agent_actions[t]\n",
        "            print(f\"  time {t}: {action_names[action_idx]}\")\n",
        "        print()\n",
        "\n",
        "def convert_State_sequence(info_State):\n",
        "    num_timesteps = info_State.time.shape[0]\n",
        "    \n",
        "    state_seq = []\n",
        "    for t in range(num_timesteps):\n",
        "        state_t = jtu.tree_map(\n",
        "            lambda x: x[t] if x.ndim == 1 else x[:, t, ...],\n",
        "            info_State\n",
        "        )\n",
        "        state_seq.append(state_t)\n",
        "    \n",
        "    return state_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 non-ToM agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agents_2nontoms = OvercookedAgent(model, batch_size = num_agents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = jr.PRNGKey(1)\n",
        "\n",
        "last, info_nontom, env_final = rollout_nontom(\n",
        "    agents_2nontoms, env, timesteps, key, \n",
        "    policy_search=tree_search_nontom\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_actions(info_nontom[\"action\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_seq = convert_State_sequence(info_nontom[\"env_state\"])\n",
        "viz = OvercookedVisualizer()\n",
        "viz.animate(state_seq, agent_view_size=5, filename='ovc1_englishmen_nontom.gif', fps=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "info_nontom[\"observation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_idx = 0\n",
        "visualize_plan_tree(info_nontom, time_idx=0, agent_idx=0, model=model, min_prob=0.1, depth=4, fig_size = (8,10), root_idx=root_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_idx = 0\n",
        "visualize_plan_tree(info_nontom, time_idx=0, agent_idx=1, model=model, min_prob=0.1, depth=2, fig_size = (8,5), root_idx=root_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 non-ToM agent, 1 ToM agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_1nontom = OvercookedAgent(model, batch_size = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "obs_mapping = jnp.array([[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4]]])# TODO add the remaining self state factors obs for the other agent: facing location and carrying state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_mapping = [[[-1, -1, -1, -1, 0]]]\n",
        "# state_mapping = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_1nontom = OvercookedAgent(model, batch_size = 1)\n",
        "\n",
        "focal_agent = ToMify(\n",
        "    agent_1nontom,\n",
        "    self_states=[0, 1, 2],\n",
        "    # world_states=list(range(3,len(model.B.keys()))),\n",
        "    world_states=[],\n",
        "    observation_mappings=obs_mapping,\n",
        "    state_mappings=state_mapping,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "other_agent = OvercookedAgent(model, batch_size = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = jr.PRNGKey(1)\n",
        "last, info_1tom, env = rollout_tom(\n",
        "    focal_agent,\n",
        "    other_agent,\n",
        "    env,\n",
        "    timesteps,\n",
        "    key,\n",
        "    other_agent_policy_search=tree_search_nontom,\n",
        "    focal_agent_tom_policy_search=tree_search_tom,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_seq = convert_State_sequence(info_1tom[\"env_state\"])\n",
        "viz = OvercookedVisualizer()\n",
        "viz.animate(state_seq, agent_view_size=5, filename='ovc1_englishmen_1tom.gif', fps=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_actions(info_1tom[\"action\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_idx = 0\n",
        "visualize_plan_tree(info_1tom, time_idx=0, agent_idx=0, model=model, min_prob=0.0, depth=2, fig_size = (8,5), root_idx=root_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_idx = 4\n",
        "visualize_plan_tree(info_1tom, time_idx=0, agent_idx=0, model=model, min_prob=0.0, depth=4, fig_size = (8,10), root_idx=root_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_idx = 99\n",
        "visualize_plan_tree(info_1tom, time_idx=0, agent_idx=0, model=model, min_prob=0.0, depth=4, fig_size = (8,10), root_idx=root_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_idx = 0\n",
        "visualize_plan_tree(info_1tom, time_idx=0, agent_idx=1, model=model, min_prob=0.0, depth=2, fig_size = (8,5), root_idx=root_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 ToM agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "obs_mapping = jnp.array([list(range(len(model.A.keys()))), list(range(len(model.A.keys())))])\n",
        "obs_mapping = jnp.repeat(obs_mapping[None, ...], 2, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_mapping = [[[-1, -1, -1, -1, 0]], [[-1, -1, -1, -1, 0]]]\n",
        "# state_mapping = jnp.repeat(state_mapping[None, ...], 2, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "focal_agents = ToMify(\n",
        "    agent_1nontom,\n",
        "    self_states=[0, 1, 2],\n",
        "    # world_states=list(range(3,len(model.B.keys()))),\n",
        "    world_states=[],\n",
        "    observation_mappings=obs_mapping,\n",
        "    state_mappings=state_mapping,\n",
        "    batch_size=num_agents\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = jr.PRNGKey(1)\n",
        "last, info_2toms, env_final = rollout_tom(\n",
        "    focal_agents, \n",
        "    other_agents=None,\n",
        "    env=env, \n",
        "    num_timesteps=timesteps, \n",
        "    rng_key=key, \n",
        "    focal_agent_tom_policy_search=tree_search_tom,\n",
        "    other_agent_policy_search=tree_search_nontom)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_seq = convert_State_sequence(info_2toms[\"env_state\"])\n",
        "viz = OvercookedVisualizer()\n",
        "viz.animate(state_seq, agent_view_size=5, filename='ovc1_englishmen_2toms.gif', fps=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_actions(info_2toms[\"action\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_idx = 0\n",
        "visualize_plan_tree(info_2toms, time_idx=0, agent_idx=0, model=model, min_prob=0.0, depth=2, fig_size = (8,5), root_idx=root_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_idx = 4\n",
        "visualize_plan_tree(info_2toms, time_idx=0, agent_idx=0, model=model, min_prob=0.0, depth=2, fig_size = (8,5), root_idx=root_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_idx = 0\n",
        "visualize_plan_tree(info_2toms, time_idx=0, agent_idx=1, model=model, min_prob=0.0, depth=2, fig_size = (8,5), root_idx=root_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_idx = 4\n",
        "visualize_plan_tree(info_2toms, time_idx=0, agent_idx=1, model=model, min_prob=0.0, depth=2, fig_size = (8,5), root_idx=root_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DEBUG ZONE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "def print_beliefs(model, qs, belief_idx):\n",
        "    state_factor_names = list(model.B.keys())\n",
        "    state_factor_name = state_factor_names[belief_idx]\n",
        "    \n",
        "    labels = model.B[state_factor_name].batch[state_factor_name]\n",
        "    \n",
        "    for agent_idx, agent_beliefs in enumerate(qs[belief_idx]):\n",
        "        print(f\"AGENT {agent_idx} {state_factor_name} BELIEFS:\")\n",
        "        \n",
        "        for t in range(agent_beliefs.shape[0]):\n",
        "            most_likely_idx = jnp.argmax(agent_beliefs[t])\n",
        "            probability = agent_beliefs[t, 0, most_likely_idx]\n",
        "            print(f\"  time {t}: {labels[most_likely_idx]} (prob={probability:.3f})\")\n",
        "        print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print_beliefs(model, info_tom[\"qs\"], belief_idx=0)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
